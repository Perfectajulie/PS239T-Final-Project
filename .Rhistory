MathAchSchool[1:10,]
# Recalculate the mean of SES for each school, save in MSES vector
attach(MathAchieve)
mses <- tapply(SES, School, mean) # attach the dataset so we don't have to use MathAchieve$SES, MathAchieve$School
mses[as.character(MathAchSchool$School[1:10])]
detach(MathAchieve)
View(MathAchieve)
View(MathAchSchool)
# Now we are making a new data frame that has all the data.
Bryk <- as.data.frame(MathAchieve[, c("School", "SES", "MathAch")])
names(Bryk) <- c("school", "ses", "mathach")
sample20 <- sort(sample(nrow(Bryk), 20)) # 20 randomly sampled students
Bryk[sample20,]
View(Bryk)
# Now we add the data that is the same for all schools
Bryk$meanses <- mses[as.character(Bryk$school)] # group level variation
View(Bryk)
Bryk$school
mses[(Bryk$school(1))]
as.character.mses[(Bryk$school(1))]
mses[as.character(Bryk$school(1))]
mses['8627']
mses[8627]
cat <- sample(unique(Bryk$school[Bryk$sector=='Catholic']), 20)
Cat.20 <- groupedData(mathach ~ ses | school, data=Bryk[is.element(Bryk$school, cat),])
pub <- sample(unique(Bryk$school[Bryk$sector=='Public']), 20)
Pub.20 <- groupedData(mathach ~ ses | school, data=Bryk[is.element(Bryk$school, pub),])
cat <- sample(unique(Bryk$school[Bryk$sector=='Catholic']), 20)
View(Bryk)
# Now we add the data that is the same for all schools
Bryk$meanses <- mses[as.character(Bryk$school)] # group level variation; using as.character turns the school number into a label; you can put row number or name of row inside square braket
Bryk$cses <- Bryk$ses - Bryk$meanses # variation within school from group level mean
sector <- MathAchSchool$Sector
names(sector) <- row.names(MathAchSchool)
Bryk$sector <- sector[as.character(Bryk$school)]
Bryk[sample20,]
cat <- sample(unique(Bryk$school[Bryk$sector=='Catholic']), 20)
Cat.20 <- groupedData(mathach ~ ses | school, data=Bryk[is.element(Bryk$school, cat),])
pub <- sample(unique(Bryk$school[Bryk$sector=='Public']), 20)
Pub.20 <- groupedData(mathach ~ ses | school, data=Bryk[is.element(Bryk$school, pub),])
trellis.device(color=F)
xyplot(mathach ~ ses | school, data=Cat.20, main="Catholic",
panel=function(x, y)	{
panel.xyplot(x, y)
panel.loess(x, y, span=1)
panel.lmline(x, y, lty=2)
}
)
trellis.device(color=F)
xyplot(mathach ~ ses | school, data=Pub.20, main="Public",
panel=function(x, y)	{
panel.xyplot(x, y)
panel.loess(x, y, span=1)
panel.lmline(x, y, lty=2)
}
)
# lm list generates a list of linear models - in this case for each school
cat.list <- lmList(mathach ~ ses | school, subset = sector=='Catholic', data=Bryk)
pub.list <- lmList(mathach ~ ses | school, subset = sector=='Public', data=Bryk)
trellis.device(color=F)
plot(intervals(cat.list), main='Catholic')
# lm list generates a list of linear models - in this case for each school
cat.list <- lmList(mathach ~ ses | school, subset = sector=='Catholic', data=Bryk)
pub.list <- lmList(mathach ~ ses | school, subset = sector=='Public', data=Bryk)
trellis.device(color=F)
plot(intervals(cat.list), main='Catholic')
trellis.device(color=F)
plot(intervals(pub.list), main='Public')
# Make a box plot of coefficients.
# First store the coefficients in a new array
cat.coef <- coef(cat.list)
cat.coef[1:10,]
pub.coef <- coef(pub.list)
pub.coef[1:10,]
detach("package:lmerTest", unload=TRUE)
# lm list generates a list of linear models - in this case for each school
cat.list <- lmList(mathach ~ ses | school, subset = sector=='Catholic', data=Bryk)
pub.list <- lmList(mathach ~ ses | school, subset = sector=='Public', data=Bryk)
trellis.device(color=F)
plot(intervals(cat.list), main='Catholic')
cat <- sample(unique(Bryk$school[Bryk$sector=='Catholic']), 20)
Cat.20 <- groupedData(mathach ~ ses | school, data=Bryk[is.element(Bryk$school, cat),])
pub <- sample(unique(Bryk$school[Bryk$sector=='Public']), 20)
Pub.20 <- groupedData(mathach ~ ses | school, data=Bryk[is.element(Bryk$school, pub),])
trellis.device(color=F)
plot(intervals(cat.list), main='Catholic')
library(nlme)
library(lme4)
#install.packages("lmerTest")
#library(lmerTest)
library(lattice)
# Now that we understand the data we can fit a hierarchical linear model
bryk.lme.1 <- lme(mathach ~ meanses*cses + sector*cses, random = ~ cses | school, data=Bryk) # DOES THE EFFECT OF WITHIN SCHOOL SES DEPEND ON BETWEEN SCHOOL SES; is there is difference in within school SES by sector; random slope and intercept by school
summary(bryk.lme.1)
2[-23251.83 + 10]
2[-23251.83] + 10
#test it
example_contents<-scrape_article_contents("https://www.nytimes.com/2018/02/18/nyregion/african-immigrants-bronx-community-college.html")
scrape_article_contents<-function(url) {
NYT_archive1 <- read_html(url)
#NYT_archives <- read_html(COP.articles$web_url)
NYT_art_content <- NYT_archive1 %>% html_nodes(css = "p")
# alternative code: article_content <- NYT_art_content %>% html_nodes(css = ".story-content")
#NYT_art_content <- NYT_archive1 %>% html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "story-content", " " ))]')
# Create empty placeholder for my NYT articles
Article_contents <- vector("list", length = length(NYT_archive1))
# Create function to save document names and urls
for (i in seq_along(NYT_art_content)) {
docname <- xml_contents(NYT_art_content[i]) %>%
html_text(trim = TRUE)
url <- html_attr(NYT_art_content[i], "div p")
#Article_contents[[i]] <- data_frame(docname = docname, url = url)
Article_contents[[i]] <- docname
}
final_art <-  paste(Article_contents, sep = " ", collapse = "")
#nyt.review.out<-getURLContent(as.character(COP.articles[1]),curl=getCurlHandle())
#a2 <- htmlTreeParse(nyt.review.url)
# article_df <- as.character(Article_contents)
return(final_art)
}
#test it
example_contents<-scrape_article_contents("https://www.nytimes.com/2018/02/18/nyregion/african-immigrants-bronx-community-college.html")
library(tidyr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(plyr)
library(rvest)
#test it
example_contents<-scrape_article_contents("https://www.nytimes.com/2018/02/18/nyregion/african-immigrants-bronx-community-college.html")
show(example_contents)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
})
all_content
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) NULL)
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# Create total hits variable
total_hits <- COP.response.df$response$meta$hits
#1015
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) NULL)
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# Write a function to create get requests
COP.api<-function(COP.search.terms=NULL, begin.date=NULL, end.date=NULL, page=NULL,
base.url="http://api.nytimes.com/svc/search/v2/articlesearch",
response.format=".json",
key="13ff759582b04c6d8050e178b2dc8d0e"){
# Combine parameters
params<-list(
c("q", COP.search.terms),
c("begin_date", begin.date),
c("end_date", end.date),
c("page", page)
)
params<-params[sapply(X = params, length)>1]
params<-sapply(X = params, FUN = paste0, collapse="=")
params<-paste0(params, collapse="&")
# URL encode query portion
query<-URLencode(URL = params, reserved = FALSE)
# Combine with base url and other options
get.request<-paste0(base.url, response.format, "?", query, "&api-key=", key)
# Send GET request
response<-httr::GET(url = get.request)
# Parse response to JSON
response<-httr::content(response, "text")
response<-jsonlite::fromJSON(txt = response, simplifyDataFrame = T, flatten = T)
return(response)
}
# Create total hits variable
total_hits <- COP.response.df$response$meta$hits
#1015
NYT.key <-"5756cf3d791442b490c1972c80d2c5a8" #from NYT
base.url<-"https://api.nytimes.com/svc/search/v2/articlesearch"
response.format<-".json"
COP.search.term<-"community policing"
# Specify and encode filters (fq)
COP.filter.query<-"body:\"community policing\""
print(COP.filter.query)
cat(COP.filter.query)
# URL-encode the search and its filters
COP.search.term<-URLencode(URL = COP.search.term, reserved = TRUE)
COP.filter.query<-URLencode(URL = COP.filter.query, reserved = TRUE)
print(COP.search.term)
print(COP.filter.query)
# Paste components together to create URL for get request
get.COP.request<-paste0(base.url, response.format, "?", "q=", COP.search.term, "&fq=", COP.filter.query, "&api-key=", NYT.key)
print(get.COP.request)
# Send the GET request using httr package
NYT.COP.response<-httr::GET(url = get.COP.request)
print(NYT.COP.response)
# Inspect the content of the response, parsing the result as text
NYT.COP.response <-httr::content(x = NYT.COP.response, as = "text")
substr(x = NYT.COP.response, start = 1, stop = 1000)
# Convert JSON response to a dataframe
COP.response.df<-jsonlite::fromJSON(txt = NYT.COP.response, simplifyDataFrame = TRUE, flatten = TRUE)
# Inspect the dataframe
str(COP.response.df, max.level = 3)
# Get number of hits
print(COP.response.df$response$meta$hits)
# Write a function to create get requests
COP.api<-function(COP.search.terms=NULL, begin.date=NULL, end.date=NULL, page=NULL,
base.url="http://api.nytimes.com/svc/search/v2/articlesearch",
response.format=".json",
key="13ff759582b04c6d8050e178b2dc8d0e"){
# Combine parameters
params<-list(
c("q", COP.search.terms),
c("begin_date", begin.date),
c("end_date", end.date),
c("page", page)
)
params<-params[sapply(X = params, length)>1]
params<-sapply(X = params, FUN = paste0, collapse="=")
params<-paste0(params, collapse="&")
# URL encode query portion
query<-URLencode(URL = params, reserved = FALSE)
# Combine with base url and other options
get.request<-paste0(base.url, response.format, "?", query, "&api-key=", key)
# Send GET request
response<-httr::GET(url = get.request)
# Parse response to JSON
response<-httr::content(response, "text")
response<-jsonlite::fromJSON(txt = response, simplifyDataFrame = T, flatten = T)
return(response)
}
# Create total hits variable
total_hits <- COP.response.df$response$meta$hits
#1015
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) NULL)
return(response)
})
# Geoff, force fewer than 1000+ pages here
total_hits<-10
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) NULL)
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# Geoff, force fewer than 1000+ pages here
total_hits<-10
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) print(e))
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# Geoff, force fewer than 1000+ pages here
total_hits<-10
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) print(e))
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
library(RJSONIO)
library (RCurl)
#NYT_article <- select(COP.articles, contains("ur"))
#test it
example_contents<-scrape_article_contents("https://www.nytimes.com/2018/02/18/nyregion/african-immigrants-bronx-community-college.html")
view(example_contents)
show(example_contents)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
})
all_content
# Geoff, force fewer than 1000+ pages here
total_pages<-total_hits/50
# to be safe
total_pages<-5
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_pages, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) {
print(e)
return(NULL)
}
)
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
result<-tryCatch(expr = function(row_index) {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
},
error = function(e) {
return(NULL)
}
)
return(result)
})
all_content
show(all_content)
View(all_content)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
result<-tryCatch(expr = {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
},
error = function(e) {
return(NULL)
}
)
return(result)
})
all_content
View(all_content)
View(all_content)
print("error " + 10)
print("error "w 10)
print("error ", 10)
sprintf("error %d", 20)
View(all_content)
View(all_content)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
entries_to_fetch<- length(COP.articles)
all_content <- sapply(X = 1:entries_to_fetch, FUN = function(row_index) {
result<-tryCatch(expr = {
# Get the URL form dataframe
url <- COP.articles$web_url[row_index];
# Return the scraped contents (as a row in our resulting dataframe)
return(scrape_article_contents(url))
},
error = function(e) {
# For debugging, why did this row fail
sprintf("error on row %d", row_index)
print(e)
# Return null so we can filter this row out
return(NULL)
})
return(result)
})
all_content
View(all_content)
View(all_content)
View(COP.articles)
length(COP.articles$web_url)
View(all_content)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
entries_to_fetch<- length(COP.articles$web_url)
all_content <- sapply(X = 1:entries_to_fetch, FUN = function(row_index) {
result<-tryCatch(expr = {
# Get the URL form dataframe
url <- COP.articles$web_url[row_index];
# Return the scraped contents (as a row in our resulting dataframe)
return(scrape_article_contents(url))
},
error = function(e) {
# For debugging, why did this row fail
sprintf("error on row %d", row_index)
print(e)
# Return null so we can filter this row out
return(NULL)
})
return(result)
})
all_content
all_content
View(all_content)
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator
#install.packages("RColorBrewer") # color palettes
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
all.art.vec <- VectorSource(all_content)
all.art.corpus <- Corpus(all.art.vec)
summary(all.art.corpus)
all.art.corpus <- tm_map(all.art.corpus, content_transformer(tolower))
all.art.corpus <- tm_map(all.art.corpus, removePunctuation)
all.art.corpus <- tm_map(all.art.corpus, removeNumbers)
all.art.corpus <- tm_map(all.art.corpus, removeWords, stopwords("english"))
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byletterto  editorcre")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byoped contributorscby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byoped contributorcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported bysunday routinecby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementc")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementfollow")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "neediest cases fund cby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "usget  upshot   inboxc")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "bycby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "byeditorialcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "new york time opinion section facebook twitter nytopinion sign opinion today newsletteradvertisementcharactercharactercharactercollapsese option")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "full story  advertisementcharactercharactercharactercollapsesee  options")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "upshot provides news analysis  graphics  politics policy  everyday life follow us  facebook  twitter  sign    newsletter cwe’re interested   feedback   page tell us   thinkgo  home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcharactercharactercharactercollapses option")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "order reprint today papersubscrib interest feedback page tell thinkgo home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "studyingadvertisementopinion")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "themadvertisement")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "society”advertisementthi")
# Stems words
# all.art.corpus <- tm_map(all.art.corpus, stemDocument)
# Remove white space
all.art.corpus <- tm_map(all.art.corpus, stripWhitespace)
#Inspect
inspect(all.art.corpus[12])
inspect(all.art.corpus[2])
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcharactercharactercharactercollapsesee options")
inspect(all.art.corpus[12])
inspect(all.art.corpus[2])
TDM <- TermDocumentMatrix(all.art.corpus)
TDM
inspect(TDM[1:10,1:10])
#Find the most common words
findFreqTerms(TDM, 150)
#Find associations
findAssocs(TDM, "african", 0.7)
# Remove sparse terms
TDM.common <-  removeSparseTerms(TDM, 0.001)
dim(TDM.common)
# inspect(TDM.common[1:10,1:10])
TDM.dense <- as.matrix(TDM)
TDM.dense
object.size(TDM.dense)
#palette <- brewer.pal(9,"BuGn")
#palette <- palette[-(1:4)]
wordcloud(rownames(TDM.dense), rowSums(TDM.dense), scale=c(5,1.5), min.freq = 1, max.words = 50, random.color = FALSE)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(plyr)
library(rvest)
library(splitstackshape)
library(stm)
library(readxl)
library(RJSONIO)
library (RCurl)
library(RTextTools)
library(data.table)
library(qdap)
library(qdapDictionaries)
library(tm)
library(SnowballC)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
setwd("~/PS239T-Final-Project")
