params<-params[sapply(X = params, length)>1]
params<-sapply(X = params, FUN = paste0, collapse="=")
params<-paste0(params, collapse="&")
# URL encode query portion
query<-URLencode(URL = params, reserved = FALSE)
# Combine with base url and other options
get.request<-paste0(base.url, response.format, "?", query, "&api-key=", key)
# Send GET request
response<-httr::GET(url = get.request)
# Parse response to JSON
response<-httr::content(response, "text")
response<-jsonlite::fromJSON(txt = response, simplifyDataFrame = T, flatten = T)
return(response)
}
# Create total hits variable
total_hits <- COP.response.df$response$meta$hits
#1015
NYT.key <-"5756cf3d791442b490c1972c80d2c5a8" #from NYT
base.url<-"https://api.nytimes.com/svc/search/v2/articlesearch"
response.format<-".json"
COP.search.term<-"community policing"
# Specify and encode filters (fq)
COP.filter.query<-"body:\"community policing\""
print(COP.filter.query)
cat(COP.filter.query)
# URL-encode the search and its filters
COP.search.term<-URLencode(URL = COP.search.term, reserved = TRUE)
COP.filter.query<-URLencode(URL = COP.filter.query, reserved = TRUE)
print(COP.search.term)
print(COP.filter.query)
# Paste components together to create URL for get request
get.COP.request<-paste0(base.url, response.format, "?", "q=", COP.search.term, "&fq=", COP.filter.query, "&api-key=", NYT.key)
print(get.COP.request)
# Send the GET request using httr package
NYT.COP.response<-httr::GET(url = get.COP.request)
print(NYT.COP.response)
# Inspect the content of the response, parsing the result as text
NYT.COP.response <-httr::content(x = NYT.COP.response, as = "text")
substr(x = NYT.COP.response, start = 1, stop = 1000)
# Convert JSON response to a dataframe
COP.response.df<-jsonlite::fromJSON(txt = NYT.COP.response, simplifyDataFrame = TRUE, flatten = TRUE)
# Inspect the dataframe
str(COP.response.df, max.level = 3)
# Get number of hits
print(COP.response.df$response$meta$hits)
# Write a function to create get requests
COP.api<-function(COP.search.terms=NULL, begin.date=NULL, end.date=NULL, page=NULL,
base.url="http://api.nytimes.com/svc/search/v2/articlesearch",
response.format=".json",
key="13ff759582b04c6d8050e178b2dc8d0e"){
# Combine parameters
params<-list(
c("q", COP.search.terms),
c("begin_date", begin.date),
c("end_date", end.date),
c("page", page)
)
params<-params[sapply(X = params, length)>1]
params<-sapply(X = params, FUN = paste0, collapse="=")
params<-paste0(params, collapse="&")
# URL encode query portion
query<-URLencode(URL = params, reserved = FALSE)
# Combine with base url and other options
get.request<-paste0(base.url, response.format, "?", query, "&api-key=", key)
# Send GET request
response<-httr::GET(url = get.request)
# Parse response to JSON
response<-httr::content(response, "text")
response<-jsonlite::fromJSON(txt = response, simplifyDataFrame = T, flatten = T)
return(response)
}
# Create total hits variable
total_hits <- COP.response.df$response$meta$hits
#1015
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) NULL)
return(response)
})
# Geoff, force fewer than 1000+ pages here
total_hits<-10
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) NULL)
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# Geoff, force fewer than 1000+ pages here
total_hits<-10
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) print(e))
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# Geoff, force fewer than 1000+ pages here
total_hits<-10
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_hits, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) print(e))
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
library(RJSONIO)
library (RCurl)
#NYT_article <- select(COP.articles, contains("ur"))
#test it
example_contents<-scrape_article_contents("https://www.nytimes.com/2018/02/18/nyregion/african-immigrants-bronx-community-college.html")
view(example_contents)
show(example_contents)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
})
all_content
# Geoff, force fewer than 1000+ pages here
total_pages<-total_hits/50
# to be safe
total_pages<-5
# Begin my stuff
# Get all articles
COP.articles<-sapply(X =0:total_pages, FUN = function(page){
#cat(page, "")
response<-tryCatch(expr = {
r<-COP.api(COP.search.terms = "community policing", begin.date = 19940101, end.date = 20180301, page = page)
r$response$docs
}, error=function(e) {
print(e)
return(NULL)
}
)
return(response)
})
# Combine list of dataframes
COP.articles<-COP.articles[!sapply(X = COP.articles, FUN = is.null)]
COP.articles<-dplyr::bind_rows(COP.articles)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
result<-tryCatch(expr = function(row_index) {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
},
error = function(e) {
return(NULL)
}
)
return(result)
})
all_content
show(all_content)
View(all_content)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
all_content <- sapply(X = 1:210, FUN = function(row_index) {
result<-tryCatch(expr = {
url <- COP.articles$web_url[row_index];
return(scrape_article_contents(url))
},
error = function(e) {
return(NULL)
}
)
return(result)
})
all_content
View(all_content)
View(all_content)
print("error " + 10)
print("error "w 10)
print("error ", 10)
sprintf("error %d", 20)
View(all_content)
View(all_content)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
entries_to_fetch<- length(COP.articles)
all_content <- sapply(X = 1:entries_to_fetch, FUN = function(row_index) {
result<-tryCatch(expr = {
# Get the URL form dataframe
url <- COP.articles$web_url[row_index];
# Return the scraped contents (as a row in our resulting dataframe)
return(scrape_article_contents(url))
},
error = function(e) {
# For debugging, why did this row fail
sprintf("error on row %d", row_index)
print(e)
# Return null so we can filter this row out
return(NULL)
})
return(result)
})
all_content
View(all_content)
View(all_content)
View(COP.articles)
length(COP.articles$web_url)
View(all_content)
# sapply loop for all articles
#all_content <- sapply(X = 1:length(COP.articles$web_url), FUN = function(row_index) {
entries_to_fetch<- length(COP.articles$web_url)
all_content <- sapply(X = 1:entries_to_fetch, FUN = function(row_index) {
result<-tryCatch(expr = {
# Get the URL form dataframe
url <- COP.articles$web_url[row_index];
# Return the scraped contents (as a row in our resulting dataframe)
return(scrape_article_contents(url))
},
error = function(e) {
# For debugging, why did this row fail
sprintf("error on row %d", row_index)
print(e)
# Return null so we can filter this row out
return(NULL)
})
return(result)
})
all_content
all_content
View(all_content)
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator
#install.packages("RColorBrewer") # color palettes
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
all.art.vec <- VectorSource(all_content)
all.art.corpus <- Corpus(all.art.vec)
summary(all.art.corpus)
all.art.corpus <- tm_map(all.art.corpus, content_transformer(tolower))
all.art.corpus <- tm_map(all.art.corpus, removePunctuation)
all.art.corpus <- tm_map(all.art.corpus, removeNumbers)
all.art.corpus <- tm_map(all.art.corpus, removeWords, stopwords("english"))
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byletterto  editorcre")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byoped contributorscby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byoped contributorcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported bysunday routinecby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementc")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementfollow")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "neediest cases fund cby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "usget  upshot   inboxc")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "bycby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "byeditorialcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "new york time opinion section facebook twitter nytopinion sign opinion today newsletteradvertisementcharactercharactercharactercollapsese option")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "full story  advertisementcharactercharactercharactercollapsesee  options")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "upshot provides news analysis  graphics  politics policy  everyday life follow us  facebook  twitter  sign    newsletter cwe’re interested   feedback   page tell us   thinkgo  home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcharactercharactercharactercollapses option")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "order reprint today papersubscrib interest feedback page tell thinkgo home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "studyingadvertisementopinion")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "themadvertisement")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "society”advertisementthi")
# Stems words
# all.art.corpus <- tm_map(all.art.corpus, stemDocument)
# Remove white space
all.art.corpus <- tm_map(all.art.corpus, stripWhitespace)
#Inspect
inspect(all.art.corpus[12])
inspect(all.art.corpus[2])
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcharactercharactercharactercollapsesee options")
inspect(all.art.corpus[12])
inspect(all.art.corpus[2])
TDM <- TermDocumentMatrix(all.art.corpus)
TDM
inspect(TDM[1:10,1:10])
#Find the most common words
findFreqTerms(TDM, 150)
#Find associations
findAssocs(TDM, "african", 0.7)
# Remove sparse terms
TDM.common <-  removeSparseTerms(TDM, 0.001)
dim(TDM.common)
# inspect(TDM.common[1:10,1:10])
TDM.dense <- as.matrix(TDM)
TDM.dense
object.size(TDM.dense)
#palette <- brewer.pal(9,"BuGn")
#palette <- palette[-(1:4)]
wordcloud(rownames(TDM.dense), rowSums(TDM.dense), scale=c(5,1.5), min.freq = 1, max.words = 50, random.color = FALSE)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(plyr)
library(rvest)
library(splitstackshape)
library(stm)
library(readxl)
library(RJSONIO)
library (RCurl)
library(RTextTools)
library(data.table)
library(qdap)
library(qdapDictionaries)
library(tm)
library(SnowballC)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
setwd("~/PS239T-Final-Project")
# Load Data
STM.df <- read.csv('NYT_corpus.csv', stringsAsFactors = FALSE)
# Pre-process
STM.COP <-textProcessor(documents=STM.df$text, metadata=STM.df)
meta<-STM.COP$meta
vocab<-STM.COP$vocab
docs<-STM.COP$documents
out <- prepDocuments(docs, vocab, meta, lower.thresh=10)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
# Model without predictor
fit0 <- stm(out$documents, # the documents
out$vocab, # the words
K = 10, # 10 topics
max.em.its = 50, # set to run for a maximum of 75 EM iterations
data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
init.type = "Spectral")
labelTopics(fit0)
plot.STM(fit0, type = "labels")
plot.STM(fit0, type = "summary")
# Model without predictor
fit0 <- stm(out$documents, # the documents
out$vocab, # the words
K = 5, # 10 topics
max.em.its = 50, # set to run for a maximum of 75 EM iterations
data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
init.type = "Spectral")
labelTopics(fit0)
plot.STM(fit0, type = "labels")
plot.STM(fit0, type = "summary")
round(topicCorr(fit0)$cor, 2)
# Load the data
all.art.corpus.df <- read.csv('All_articles.csv', stringsAsFactors = FALSE)
#all.art.corpus <- all.art.corpus[-1]
colnames(all.art.corpus.df)[2] <- "text"
# Create corpus
all.art.corpus <- Corpus(VectorSource(all.art.corpus.df$text))
# Create a function that converts some `pattern` into a single space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
#Remove the weird stuff
all.art.corpus <- tm_map(all.art.corpus, toSpace, "—") #Weird hyphen
all.art.corpus <- tm_map(all.art.corpus, toSpace, "“") # Left double quote
all.art.corpus <- tm_map(all.art.corpus, toSpace, "”") # Right double quote
all.art.corpus <- tm_map(all.art.corpus, toSpace, "’s") # Possessive
all.art.corpus <- tm_map(all.art.corpus, toSpace, "’ve") #
all.art.corpus <- tm_map(all.art.corpus, toSpace, "’re") #
all.art.corpus <- tm_map(all.art.corpus, toSpace, "’")  # The rest
# Create a function to merge New York City
toNYC <- content_transformer(function (x) gsub("new york city", "nyc", x))
all.art.corpus <- tm_map(all.art.corpus, toNYC)
#Create a function to merge New York City with New York State
toNYS <- content_transformer(function (x) gsub("new york state", "nys", x))
all.art.corpus <- tm_map(all.art.corpus, toNYs)
#Create a function to merge New York City with New York State
toNYS <- content_transformer(function (x) gsub("new york state", "nys", x))
all.art.corpus <- tm_map(all.art.corpus, toNYS)
# More text processing
all.art.corpus <- tm_map(all.art.corpus, PlainTextDocument)
all.art.corpus <- tm_map(all.art.corpus, content_transformer(tolower))
all.art.corpus <- tm_map(all.art.corpus, removePunctuation)
all.art.corpus <- tm_map(all.art.corpus, removeWords, stopwords('english'))
all.art.corpus <- tm_map(all.art.corpus, removeNumbers)
## Remove more unwanted stuff
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byletterto  editorcre")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byoped contributorscby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported byoped contributorcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported bysunday routinecby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementc")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementsupported")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementfollow")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "neediest cases fund cby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "usget  upshot   inboxc")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "bycby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "byeditorialcby")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "new york time opinion section facebook twitter nytopinion sign opinion today newsletteradvertisementcharactercharactercharactercollapsese option")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "full story  advertisementcharactercharactercharactercollapsesee  options")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "upshot provides news analysis  graphics  politics policy  everyday life follow us  facebook  twitter  sign    newsletter cwe’re interested   feedback   page tell us   thinkgo  home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcharactercharactercharactercollapses option")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "order reprint today papersubscrib interest feedback page tell thinkgo home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "studyingadvertisementopinion")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "themadvertisement")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "society”advertisementthi")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "advertisementcharactercharactercharactercollapsesee options")
all.art.corpus <- tm_map(all.art.corpus, removeWords, "international herald tribune todays papersubscribe cwe’re interested   feedback   page tell us   thinkgo  home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords,"news blog  live reporting features  reader conversations  new york city   archived send questions  suggestions  email end entryadvertisement")
all.art.corpus <- tm_map(all.art.corpus, removeWords,"weekday morning   roundup  mondays print edition   can find  latest entries  nytimescomdiary    new york section onlineccity room®")
all.art.corpus <- tm_map(all.art.corpus, removeWords,"oped appears  print    page    new york edition   headline")
all.art.corpus <- tm_map(all.art.corpus, removeWords,"nypd todays papersubscribe cwe’re interested   feedback   page tell us   thinkgo  home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords,"interested   feedback   page tell us   thinkgo  home page")
all.art.corpus <- tm_map(all.art.corpus, removeWords,"order reprints todays papersubscribe go home page")
#all.art.corpus <- tm_map(all.art.corpus, removeWords,"“")
#all.art.corpus <- tm_map(all.art.corpus, removeWords,"”")
#all.art.corpus <- tm_map(all.art.corpus, removeWords,"—")
#all.art.corpus <- tm_map(all.art.corpus, removeWords,"’s")
#all.art.corpus <- tm_map(all.art.corpus, removeWords,"’re")
#all.art.corpus <- tm_map(all.art.corpus, removeWords,"’ve")
# Remove white space
all.art.corpus <- tm_map(all.art.corpus, stripWhitespace)
# Stems words
# all.art.corpus <- tm_map(all.art.corpus, stemDocument)
#Inspect
inspect(all.art.corpus[236])
TDM <- TermDocumentMatrix(all.art.corpus)
inspect(TDM[1:10,1:10])
TDM.mtx <- as.matrix(TDM)
TDM.mtx <- tibble::rownames_to_column(as.data.frame(TDM.mtx))
names(TDM.mtx)<-c("word", "frequency")
TDM.mtx <- TDM.mtx[order(-TDM.mtx$frequency),]
colorVec = colorRampPalette(c("#151A64", "#4292C6"))(nrow(TDM.mtx))
wordcloud2(TDM.mtx,
rotateRatio = 0,
color = colorVec,
shape = 'circle')
?wordcloud2
colorVec = colorRampPalette(c("#151A64", "#4292C6"))(nrow(TDM.mtx))
wordcloud2(TDM.mtx,
rotateRatio = 0,
color = colorVec,
shape = 'cardioid')
colorVec = colorRampPalette(c("#151A64", "#4292C6"))(nrow(TDM.mtx))
wordcloud2(TDM.mtx,
rotateRatio = 0,
color = colorVec,
shape = 'circle')
wordcloud2(TDM.mtx,
color = ifelse(demoFreq[, 2] > 20, 'red', 'blue'))
TDM.mtx2 <- as.matrix(TDM)
v <- sort(rowSums(TDM.mtx2),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 25)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
col ="grey", main ="Most Frequent Words",
ylab = "Word Frequencies")
#Save as csv
TDM.2 <- as.data.frame(as.matrix(TDM))
write.csv(TDM.2,"DocTermMatrix.csv")
write.csv(all.art.corpus.df,"NYT_corpus.csv")
colorVec = colorRampPalette(c("#151A64", "#4292C6"))(nrow(TDM.mtx))
wordcloud2(TDM.mtx,
rotateRatio = 0,
color = colorVec,
shape = 'circle')
colorVec = colorRampPalette(c("#151A64", "#4292C6"))(nrow(TDM.mtx))
wordcloud2(TDM.mtx,
rotateRatio = 0,
color = colorVec,
shape = 'circle')
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
col ="grey", main =" 20 Most Frequent Words",
ylab = "Word Frequencies")
wordcloud2(TDM.mtx,
color = ifelse(demoFreq[, 2] > 20, 'red', 'blue'))
#Save as csv
TDM.2 <- as.data.frame(as.matrix(TDM))
write.csv(TDM.2,"DocTermMatrix.csv")
write.csv(all.art.corpus.df,"NYT_corpus.csv")
# Load Data
STM.df <- read.csv('NYT_corpus.csv', stringsAsFactors = FALSE)
# Pre-process
STM.COP <-textProcessor(documents=STM.df$text, metadata=STM.df)
meta<-STM.COP$meta
vocab<-STM.COP$vocab
docs<-STM.COP$documents
out <- prepDocuments(docs, vocab, meta, lower.thresh=10)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
# Model without predictor
fit0 <- stm(out$documents, # the documents
out$vocab, # the words
K = 5, # 10 topics
max.em.its = 50, # set to run for a maximum of 75 EM iterations
data = out$meta, # all the variables (we're not actually including any predictors in this model, though)
init.type = "Spectral")
labelTopics(fit0)
plot.STM(fit0, type = "labels")
plot.STM(fit0, type = "summary")
